\documentclass{mipt-thesis-ms}
% Следующие две строки нужны только для biblatex. Для inline-библиографии их следует убрать.
%\usepackage{mipt-thesis-biblatex}
%\addbibresource{example.bib}

\title{Рекомендательные системы, основанные на графовых нейронных сетях}
\author{Минеев Н.\,Р.}
\supervisor{Киселёв Д.\,А.}
\referee{Петров Д.\,Е.}       % требуется только для mipt-thesis-ms
\groupnum{M05-114a}
\faculty{Физтех-школа Прикладной Математики и Информатики}
\department{Кафедра алгоритмов и технологий программирования}


% так можно определять команду для повторяющихся обозначений,
% чтобы не набирать каждый раз заново
\newcommand{\E}{\ensuremath{\mathsf{E}}}  % матожидание
\newcommand{\D}{\ensuremath{\mathsf{D}}}  % дисперсия
\newcommand{\Prb}{\ensuremath{\mathsf{P}}}  % вероятностная мера

\newcommand{\eps}{\varepsilon}  % нормальная буква эпсилон
\renewcommand{\phi}{\varphi}  % нормальная буква фи

\renewcommand{\le}{\leqslant}  % нормальный знак <=
\renewcommand{\leq}{\leqslant}  % нормальный знак <=
\renewcommand{\ge}{\geqslant}  % нормальный знак >=
\renewcommand{\geq}{\geqslant}  % нормальный знак >=

\newtheorem{lemma}{Лемма}  % создаёт команд для лемм, можно сделать так же для любого другого вида утверждений
\newtheorem{Def}{Определение}[section]





\begin{document}

\frontmatter
\titlecontents

\mainmatter


\chapter{Введение}
Здесь идет текст. Вот так выглядит ссылка на библиографию \cite{langmuir26}. Аналогично добавляются еще главы, внутри них можно объявлять секции с помощью \verb|\section|.


\chapter{Основные понятия и определения}
В данной главе приводятся определения, описание подходов и методов

\section{Общая постановка задачи рекомендаций и методы решения}
{\bf Постановка задачи рекомендаций.} 
Задано множество пользователей $U = \{u_1, u_2, \dots, u_m\}$ и множество товаров $I = \{i_1, i_2, \dots, i_n\}$, и существует целевая функция $r: U \times I \times \mathcal{T} \rightarrow R$, выражающая результат взаимодействия пользователей с товарами, где $R$ --- линейно упорядоченное множество возможных результатов взаимодействия, $\mathcal{T}$ --- время.

Для краткости записи, введем следующие обозначения: \\
$r^t_{ui} = r(u, i, t) \in R$, \\
$r^t = r:\{t\} \times U \times I \rightarrow R$, где $t$ --- константа, \\
$r_{ui} = r: U \times I \rightarrow R$ --- отображение не зависит от $t \in \mathcal{T}$.

{\bf Задача рекомендации $K$ товаров:} По известной до момента времени $Т \in \mathcal{T}$ истории взаимодействий пользователей с товарами $\{u, i, t, r_{ui}^t\}^{t<T}_{u \in U,\:i \in I}$, необходимо 
\begin{enumerate}
    \item для момента времени $T$ построить аппроксимацию целевой функции $\hat r^T: U \times I \rightarrow R$;
    \item для каждого пользователя отранжировать товары по убыванию аппроксимированного результата взаимодействия в момент времени $Т$, взять первые $K$ товаров в качестве рекоммендованных.
\end{enumerate}

Для простоты, здесь и далее будем предполагать $К = 1$, а $R = \{0, 1\}$, где результат взаимодейтсвия $1$ будет интерпретироваться как положительный (понравилось, лайк, клик, покупка, итд), а $0$ как отрицательный (не понравилось, дизлайк, отказ от клика/покупки итд). Все приведенные в работе рассуждения несложно обобщаются на случай большего $К$ и большей мощности множества $R$, тем не менее оставим данное обобщение для будущих исследований.

{\it Для решения задачи рекомендации} применяют два основных подхода: контентный и коллаборативная фильтрация.
\\

{\bf Контентный подход решения задачи рекомендации.} В контентном подходе, пользователю рекоммендуются товары, похожие на понравившиеся ему ранее.

Пусть для каждого товара $i \in I$ имеется его признаковое описание $h_i \in \mathbb{R}^n$. Тогда для решения задачи рекомендации задаются функция аггрегации последовательности признаковых описаний товаров произвольной длины $l$, $agg: (\mathbb{R}^n)^l \rightarrow \mathbb{R}$ и, в пространстве признаковых описаний товаров, метрика $d: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$. \\
На первом шаге с помощью функции аггрегации строится профиль пользователя как аггрегация признаковых описаний товаров, с которыми данный пользователь провзаимодействовал $h_u = agg(\{h_i\})$. \\
На следующем шаге в качестве рекомендации пользователю выбирается товар, признаковое описание которого к профилю пользователя оказалось ближайшим: $i_{rec} = \arg \min_i d(h_u, h_i)$. [P. Lops, M. d. Gemmis, and G. Semeraro, “Content-based recommender systems: State of the art and trends,”
Recommender systems handbook, pp. 73–105, 2011.]
\\

{\it Основное преимущество} контентного подхода в поддержке холодного старта для товаров и несложной интерпретируемости, объяснимости рекомендаций. {\it Недостаток} --- остутствие новизны: рекомендации основываются на похожих товарах.

{\bf Метод коллаборативной фильтрации для решения задачи рекомендации.} В случае коллаборативной фильтрации, пользователю рекоммендуются товары, понравившиеся похожим на данного пользователя пользователям.

Пусть для каждого товара $i \in I$ и пользователя $u \in U$ имеется признаковое описание, соответственно, $h_i \in \mathbb{R}^n$ и $h_u \in \mathbb{R}^n$. Тогда для решения задачи рекомендации определяется понятие окрестности пользователя $S(u)$ в пространстве $U$, а также задаются функция скоринга $score: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$, преобразующая пару признаковых описаний пользователя и товара в действительный число --- оценку, характеризующую привлекательность пользователю данного товара, и функция аггрегации последовательности оценок длины $l$, $agg: \mathbb{R}^l \rightarrow \mathbb{R}$. \\
На первом шаге с помощью функций скоринга и аггрегации оценок вычисляются приближенные рейтинги пользователя для каждого товара $\hat r_{ui} = {agg}_{v \in S(u)}(score(h_v, h_i))$. \\
На следующем шаге в качестве рекомендации пользователю выбирается товар, для которого приближенный рейтинг пользователя максимальный $i_{rec} = \arg \max_i \hat r_{ui}$. [https://arxiv.org/pdf/2206.02631.pdf]
\\

{\it Основное преимущество} коллаборативной фильтрации в более высоком качестве рекомендаций, но проблему холодного старта необходимо решать отдельно.
\\

{\bf Графовая структура задачи рекомендации.}\\
{\bf Граф} --- двойка, состоящая из множества вершин $V$ и множества ребер $E \subseteq V \times V$ --- пар элементов множества вершин: \:$G = <V, E>$.\\
{\bf Нейронные сети} --- метод машинного обучения, позволяющий извлекать информацию из сложноструктурированных данных, подобным человеческому мозгу образом.\\
{\bf Графовые нейронные сети (GNN)} --- тип нейронных сетей, позволяющих извлекать информацию из графов. \\
В основе графовых нейронных сетей лежит парадигма\\{\bf обмена сообщениями (message passing)}:
\\На нулевой итерации для каждой вершины графа $v \in V$ иницилизируется соответствующее векторное представление $x_v^0 \in \mathbb{R}^n$.
\\На каждой последующей итерации $t+1$:
\begin{enumerate}
    \item с помощью функции сообщений $msg$ для каждого ребра графа $e = (u, v) \in E$ вычисляется сообщение: $m_e^{t+1} = msg(x_u^t, x_v^t, w_e^t)$
    \item для каждой вершины графа $v \in V$ вычисляется обновленное векторное представление с помощью функции аггрегации сообщений от ребер $agg$, инцидентных данной вершине, и функции обновления $upd$: $x_v^{t+1} = upd(x_v^t, agg(\{m_e^{t+1}: (u, v) \in E\}))$
\end{enumerate}

Таким образом, по окончании $k$-итерационной процедуры векторные представления вершин содержат информацию о своих соседях до $k$го порядка. Часто, итерации называют слоями, а саму графовую нейронную сеть, как следствие, $k$-слойной.
\\

В силу того, что процесс взаимодействия пользователей и товаров имеет графовую структуру, а именно, представим в виде двудольного графа, который будем называть {\bf графом взаимодействий пользователей и товаров}, где вершины --- пользователи и товары, а ребра --- взаимодействия: $G = <U \cup I, r>$, для решения задачи рекомендации возможно применение графовых нейронных сетей следующим образом:
\begin{enumerate}
\item Необходимо с помощью графовой нейронной сети для графа взаимодействий пользователей и товаров вычислить векторные представления вершин, соответственно, векторные представления пользователей и товаров: $h_u$ и $h_i$.
\item С помощью скоринговой функции $score$ вычислить приближенные рейтинги пользователя для каждого товара $\hat r_{ui} = score(h_v, h_i)$.
\item в качестве рекомендации пользователю выбирается товар, для которого приближенный рейтинг пользователя максимальный $i_{rec} = \arg \max_i \hat r_{ui}$
\end{enumerate}

Таким образом, вычисленные с помощью графовых нейронных сетей на первом шаге, векторные представления пользователей, благодаря парадигме обмена сообщений, лежащей в основе GNN, будут содержать информацию как о товарах, с которыми провзаимодействовал данный пользователь, так и о других пользователях, которые также провзаимодействовали с данными товарами. То же и для товаров, их векторные представления будут содержать информацию как о пользователях, которые провзаимодействовали с этим товаром, так и о других товарах, с которыми данные пользователи также провзаимодейcтвовали.

Соответственно, {\it основное преимущество} решений задачи рекомендации, основанных на графовых нейронных сетях, является применение одновременно обоих подходов, контентного и коллаборативной фильтрации, благодаря лежащей в основе графовых нейронных сетей парадигме обмена сообщений.

{\it Важной особенностью} графа взаимодействий пользователей и товаров является зависимость его структуры от времени. Между пользователями и товарами возникают новые взаимодействия, количество пользователей и товарой может расти и убывать, что в структуре графа выражается в виде добавления/удаления ребер и вершин. Исходя из этого, для достижения выского качества решения задачи рекомендаций необходимо не только извлекать информацию из графовой стркутуры, но и учитывать её динамику.

\section{Обучение представлений статических графов}
В случае графов, структура которых статична, для вычисления векторных представлений вершин используют статичные графовые нейронные сети. В их основе лежит описанная в предыдущей секции парадигма обмена сообщений, а различия, в основном, заключены в используемых функциях сообщений, агрегации и обновления. Рассмотрим статические графовые нейронные сети, используемые в данной работе.\\

Graph Convolutional Network. Для аггрегации информации от соседей вершины и итеративного обновления векторных представлений использует спектральное разложение Лапласиана графа:
$$m_{uv} = d_{vv}^{-\frac12}a_{uv}d_{uu}^{-\frac{1}{2}} h_u^l$$
$$h_v^{l+1} = \sigma\left(W^l \sum_{u \in N(v)}m_{uv}\right)$$ 
GraphSAGE.
Graph Attention Netowrk.

\section{Обучение представлений динамических графов}
Динамический граф --- граф, структура которого изменяется во времени: $G = <V(t), E(t)>,\: t \in \mathcal{T}$.
Существует два основных способа представления динамических графов:
Дискретное представление динамического графа --- представление динамического графа в виде последовательности статичных графов(снэпшотов), каждый из которых является зафиксированным в определенный момент времени динамический граф.
Графовые нейронные сети для динамических графов являются обобщением статических, используя в основе похожие принципы, допольнительно учитывая временное измерение.
В том случае, когда структура графа, то есть количество 

В случае, когда структура графа может изменяться во времени изспользуют два основных представления
непрерывный и дискретный графы
в дискретном случае основным является фреймворк гцрн
в непрерывном тгн

\section{Обучение с подкреплением}
тбд


\chapter{Графовые модели для решения задачи рекомендаций онлайн}
Основным преимуществом решения задачи рекомендации в форме обучения с подкреплением является ... .

\section{Постановка задачи рекомендаций онлайн}
{\bf Постановка сессионной задачи рекомендации онлайн.} Пусть имеется множество пользователей $U = \{u_1, u_2, \dots, u_m\}$, множество товаров $I = \{i_1, i_2, \dots, i_n\}$ и существует функция $r:~U~\times~I~\times~\mathcal{T}~\rightarrow~\{0,  1\}$, выражающая результат взаимодействия пользователей с товарами так, что $r(u, i, t) = 0$ интерпретируется как <<пользователь $u$ в момент времени $t$ не удовлетворен товаром $i$>> (в то же время $r(u, i, t) = 1$ интерпретируется как <<пользователь $u$ в момент времени $t$ удовлетворен товаром $i$>>), где $\mathcal{T}$ --- время.\\
В случае сессионной постановки задачи рекомендации, когда $\mathcal{T} = \{0, 1, \dots, T - 1\}$, агент, решающий задачу рекомендации, взаимодействует с пользователем в течении $T$ временных шагов. Для каждого момента времени $t$: \\
\begin{enumerate}
\item агент наблюдает состояние $s_t$ пользователя $u$, которое представляется в виде последовательности товаров, с которыми провзаимодейтсвовал пользователь $u$ до момента времени $t$: $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ \\
\item на основании своей политики $\pi: I^t \rightarrow I$ агент выбирает действие $a_t \in I$, то есть следующий рекомендуемый пользователю товар. 
\item Далее, в момент времени $t+1$, в соответствии с выбранным действием $a_t$, агент получает награду ${reward}_t = r_t = r^{t}_{ui} = r(u, i, t)$ от пользователя $u$ и наблюдает новое состояние $s_{t+1} = \{a_0, a_1, \dots, a_{t}\}$ пользователя $u$. 
\end{enumerate}

Для оценки качества решения задачи онлайн рекомендации в сессионной постановке будем использовать {\bf среднюю награду за сессию (Average Reward)}: $AverageReward = \frac1T \sum_t r_t$.\\

Таким образом, {\bf сессионная задача рекомендации онлайн} формулируется следующим образом:\\
Пусть дано множество пользователей для обучения $U_{train} \subseteq U$. Необходимо по взаимодейтсвиям с пользователями из $U_{train}$ построить политику $\pi$, которая для любого нового пользователя $u \in U_{test} \subseteq U$ максимизирует среднюю награду за сессию $AverageReward \rightarrow \max$.

Для решения задачи будем использовать {\bf метод Q-обучения}: \\Выразим политику через Q-функцию $\pi(s) = \arg \max Q^*(s, a)$. Тогда для решения задачи, необходимо построить аппроксимацию оптимальной Q-функции с помощью модели машинного обучения $Q^*(s, a) \approx Q_{\theta}(s, a)$, где $\theta$ -- параметры модели.


\section{Graph Convolutional Q-Network}
В 2020 году была предложена графовая модель для решения сессионной задачи рекомендации онлайн. Векторное представление состояния пользователя вычисляется с помощью модели, основанной на GCRN-фреймворке, а именно состояние пользователя $u$, представляемое в виде последовательности товаров, с которыми данный пользователь провзаимодействовал, преобразуется в последовательность графовых векторных представлений, полученных с помощью Graph Attention Netowork (GAT), примененной к графу взаимодействий пользователей и товаров, после чего данная последовательность аггрегируется с помощью рекуррентной нейронной сети Gated Recurrent Network (GRU), вычисляя таким образом векторное представление текущего состояния пользователя, учитывающее динамику графа взаимодействий пользователей и товаров. Далее рассматривается архитектура модели подробнее.
\\

{\bf Слой векторных представлений.} Отобразим каждого пользователя $u$ и товар $i$ в векторное пространство размерности $d$, таким образом получим, соответственно, векторные представления для пользователя $e_u \in \mathbb{R}^d$ и товара $e_i \in \mathbb{R}^d$. Данные векторные представления будем рассматривать как признаки вершин в графе, они будут случайно проициализированы в начале процесса обучения и обновляться вместе с другими параметрами модели.\\

{\bf GCN-слой.} Состояние $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ пользователя $u$ представим в виде динамического графа дискретного времени $G = \{G^0, G^1, \dots, G^{t-1}\}$, где $G^{\tau}$ --- снэпшот графа взаимодействий пользователей и товаров, взятый в момент времени $\tau$. Тогда для вычисления векторного представления $h_{s_t}$ состояния пользователя $u$, учитывающего структуру графа взаимодействий пользователей и товаров и его динамику, используем GCRN фреймворк, а векторные представления действий $h_a$ вычислим, применив статичную графовую нейронную сеть к последнему снэпшоту $G^{t-1}$.\\

На первом этапе GCRN к каждому снэпшоту применяется статичная графовая нейронная сеть, в случае GCQN применяется GAT. Для каждого товара $i$ найдем его векторное графовое представление $x_i \in \mathbb{R}^d$:
$$x_i = {relu}(W_{fc}[e_i || e_{N(i)}] + b_{fc}),$$
где $W_{fc} \in \mathbb{R}^{d \times 2d}$ и $b_{fc} \in \mathbb{R}^d$ обучаемые веса и смещения полносвязного слоя, $e_i$ векторное представление товара $i$, $||$ операция конкатенации, и $e_{N(i)} \in \mathbb{R}^d$ векторное представление соседей товара $i$:
$$e_{N(i)} = \sum_{w \in N(i)} \alpha_{iw}e_w,$$
где $N(i)$ множество соседей первого порядка товара $i$ в соответствующем снэпшоте графа взаимодействий пользователей и товаров $G^{\tau}$, $e_w$ векторное представление пользователя $w$, $\alpha_{iw}$ оценка внимания (attention score), определяющая количество информации, которое будет передано векторному представлению соседей товара $i$ от соседа $w$ и вычисляющаяся следующим образом: 
$$\alpha_{iw} = \frac{\exp(w_a^T\tanh(W_a[e_i || e_w]))}{\sum_{v \in N(i)} \exp(w_a^T \tanh(W_a[e_i || e_v]))},$$
где $W_a \in \mathbb{R}^{d \times 2d}$ и $w_a \in \mathbb{R}^d$ обучаемые веса механизма внимания, $\cdot^T$ операция транспонирования.

Таким образом, для каждого товара $a_{\tau} \in s_t$, берется соответствующий снэпшот графа $G_{\tau}$, для которого применяется процедура описанная выше и вычисляется векторное графовое представление $x_{a_{\tau}}$ товара $a_{\tau}$, а вся последовательность целиком $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ преобразуется в последовательность $x_{s_t} = \{x_{a_0}, x_{a_1}, \dots, x_{a_{t-1}}\}$.\\

В качестве векторного представления $h_{a_i}$ действия $a_i \in I$ используется векторное графовое представление $x_i \in \mathbb{R}^d$ товара $i$, полученное применением описанноей выше процедуры к последнему снэпшоту $G^t$.\\

{\bf GRU-слой.} На втором этапе GCRN, к последовательности графовых векторных представлений применяют операцию аггрегации, в случае GCQN применяют GRU, для учета динамики графа.

Последовательность $x_{s_t} = \{x_{a_0}, x_{a_1}, \dots, x_{a_{t-1}}\}$ преобразуют в последовательность $\{h_0, h_1, \dots h_{t-1}\}$, где $h_j \in \mathbb{R}^d$ скрытое состояние GRU на шаге $j$: $h_j = GRU(h_{j-1}, x_{a_j})$.
Последовательность скрытых состояний, в свою очередь, аггрегируется с помощью механизма внимания, таким образом получаем векторное представление $h_{s_t}$ состояния $s_t$:
$$h_{s_t} = \sum_{j=0}^{t-1} \beta_j h_j,$$
где $\beta_j$ оценки внимания, определяющия количество информации, которое будет передано векторному представлению состояния  $h_{s_t}$ от скрытого состояния $h_j$ и вычисляющиеся следующим образом:
$$\beta_j = \frac{\exp(w_{sa}^T\tanh(W_{sa}h_j))}{\sum_{l=0}^{t-1}\exp(w_{sa}^T\tanh(W_{sa}h_l))},$$
где $W_{sa} \in \mathbb{R}^{d \times d}$ и $w_a \in \mathbb{R}^d$ обучаемые веса механизма внимания.\\

{\bf MLP-слой.} Для вычисления итогового приближенного значения Q-функции используем многослойный перцептрон (MLP), который принимает на вход конкатенацию векторных представлений состояния $h_{s_t}$ и действия $h_{a_i}$ и возвращает приближенное значение $Q_{\theta}(s_t, a_i) \approx Q^*(s_t, a_i)$.\\

Благодаря тому, что в основе GCQN лежит GCRN фреймворк, модель обладает всеми {\it преимуществами} решений задачи рекомендации, основанных на графовых нейронных сетях. Но в модели существует {\it недостаток}, внесенный GCRN, а именно, дискретный учет динамики графа взаимодействий пользователей и товаров, из-за чего, во время разбиения графа на снэпшоты, возможна потеря важной информации, что приводит, в конечном итоге, к потере качества решения задачи рекомендации. В предлaгаемой далее модели предпринята попытка данный недостаток устранить.

\section{Предлагаемая модель}

Как было заключено в предыдущей секции, в основе модели GCQN для решения сессионной задачи рекомендации онлайн, лежит графовая модель GCRN, учитывающая динамику графа взаимодействий пользователей и товаров дискретно, что потенциально может приводить к потере информации во время дискретизации и снижению качества решения задачи рекомендации. Одним из возможных способов устранить данный недостаток является внедрение в модель TGN фреймворка, учитывающего динамику графа непрерывно, вместо GCRN. Таким образом была получена новая усовершенствованная модель --- TGQN. Далее рассмотрим архитектуру модели подробнее.
\\

{\bf Слой векторных представлений.} Отобразим каждого пользователя $u$ и товар $i$ в векторное пространство размерности $d$, таким образом получим, соответственно, векторные представления для пользователя $e_u \in \mathbb{R}^d$ и товара $e_i \in \mathbb{R}^d$. Данные векторные представления будем рассматривать как признаки вершин в графе, они будут случайно проициализированы в начале процесса обучения и обновляться вместе с другими параметрами модели.\\

{\bf TGN-слой.} Состояние $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ пользователя $u$ представим в виде динамического графа непрерывного времени, то есть в виде последовательности событий $G = \{x_{u_0, i_0}(t_0), x_{u_1, i_1}(t_1), \dots, x_{u_n, i_n}(t_n)\}$, где $x_{u_k, i_k}(t_{k})$ --- событие взаимодействия в момент времени $t_k$ между пользователем $u_k$, возможно отличным от пользователя $u$, для состояния $s_t$ которого мы ищем векторное представление, и товаром $i_k$. Тогда для вычисления векторных представлений $h_{s_t}$ состояния пользователя $u$ и действий $h_a$, учитывающих структуру графа взаимодействий пользователей и товаров и его динамику, используем TGN фреймворк.\\

TGN фреймворк состоит из пяти модулей: Memory, Message Function, Message Aggregator, Memory Updater, Embedding. Рассмотрим работу каждого модуля по отдельности.\\

{\it Memory}-модуль. Для каждой вершины $v$ графа взаимодействий пользователей и товаров в момент времени $t$ memory-модуль содержит вектор памяти $s_{v}(t)$. Вектор памяти обновляется после каждого события на динамическом графе, таким образом, сохраняя в сжатом виде историю о событиях, в которых участвовала данная вершина $v$. В начале обучения все векторы памяти инициализируются нулями.\\

{\it Message Function}. Функция сообщений ${msg}$ вычисляет сообщение для каждой вершины $u$ и $i$, участвующей в событии $x_{u, i}(t)$, как конкатенацию их признаков $e_u \in \mathbb{R}^d$ и $e_i \in \mathbb{R}^d$ и векторного представления разности текущего момента времени $t$ и момента времени $t^-$ последнего события, в котором участвовала соответствующая вершина 
$$m_u(t) = [e_u || e_i || \phi(t - t^-_u)],$$
$$m_i(t) = [e_u || e_i || \phi(t - t^-_i)],$$
где $||$ --- операция конкатенации, $\phi(\cdot)$ --- функция, преобразующая разность моментов времени в вектор.\\

{\it Message Aggregator}. Так как в одном батче событий одна и та же вершина $v$ может встречаться несколько раз, соответственно, для неё будет несколько сообщений, которые необходимо аггрегировать с помощью $agg$ функции. В качестве $agg$ функции будем использовать max-pooling по времени, то есть просто вытаскивать самое последнее сообщение
$$\hat m_v(t) = \max_t(m_v(t_1), \dots, m_v(t_k))$$

{\it Memory Updater}. Данный модуль обновляет вектор памяти для вершины $v$, учавствующей в событии, на основе её аггрегированного сообщения $\hat m_v(t)$ и старого вектора памяти $s_v(t^-)$. В качестве memory updater'а будем использовать Gated Recurrent Network (GRU)
$$s_v(t) = GRU(\hat m_v(t), s_v(t^-))$$

{\it Embedding}-модуль. Для решения проблемы <<застаревания памяти>> (memory staleness problem) $s_v(t)$ используется дополнительный модуль, вычисляющий итоговое векторное представление $h_v(t) \in \mathbb{R}^d$ вершины графа $v$. В качестве embedding-модуля используем двуслойный Temporal Graph Attention: на вход каждый слой принимает представление $h_v^{l-1}(t)$ вершины $v$ с предыдущего слоя, текущий момент времени $t$, представления соседей вершины $v$ $\{h_1^{l-1}(t), \dots, h_N^{l-1}(t)\}$ вместе с моментами времени $t_1, \dots, t_N$
$$h_v^0(t) = [s_v(t) || e_v]$$
$$C^l(t) = [h_1^{l-1}(t)||\phi(t - t_1), \dots, h_N^{l-1}(t)||\phi(t - t_N)]$$
$$K^l(t) = V^l(t) = C^l(t)$$
$$q^l(t) = h_v^{l-1} || \phi(0)$$
$$\hat h_i^l(t) = MultiHeadAttention^l(q^l(t), K^l(t), V^l(t))$$
$$h_v^l(t) = MLP^l(h_v^{l-1}(t) || \hat h_v^l(t))$$
Здесь $\phi(\cdot)$ --- функция, преобразующая разность моментов времени в вектор, $||$ --- операция конкатенации, $MLP$ --- многослойный перцептрон.

Таким образом, выходные векторы $h_v^2(t) = h_v(t) \in \mathbb{R}^d$ с последнего слоя Temporal Graph Attention, примененного к графу взаимодействий пользователей и товаров, непрерывно учитывающие структуру графа и его динамику, будем использовать в качестве векторных представлений состояния $s_t = h_u(t)$ пользователя $u$ и, соответственно, действий $h_{a_i} = h_i$ товара $i$.\\

{\bf MLP-слой.} Для вычисления итогового приближенного значения Q-функции используем многослойный перцептрон (MLP), который принимает на вход конкатенацию векторных представлений состояния $h_{s_t}$ и действия $h_{a_i}$ и возвращает приближенное значение $Q_{\theta}(s_t, a_i) \approx Q^*(s_t, a_i)$.\\


Благодаря тому, что в основе TGQN лежит TGN фреймворк, модель обладает всеми {\it преимуществами} решений задачи рекомендации, основанных на графовых нейронных сетях. К тому же, модель {\it устраняет недостаток} предыдущего аналога GCQN, учитывая динамику графа взаимодействий пользователей и товаров непрерывно, не разбивая граф на снэпшоты, а обрабатывая непрерывный поток событий, что решает проблему потенциальной потери информации при дискретизации и на практике должно привести к приросту качества. Далее будет рассмотрен вычислительный эксперимент, и на основании проведенного сравнения и анализа сделанное выше предположение будет эмпирически проверено.

\chapter{Вычислительный эксперимент}
Для проверки предположения о том, что замена дискретного учета динамики графа взаимодействий пользователей и товаров на непрерывный приводит к росту качества решения задачи рекомендации, а также общей состоятельности предложенного метода относительно некоторых стандартных алгоритмов, был поставлен вычислительный эксперимент. В рамках эксперимента решалась сессионная задача рекомендации онлайн, с длиной сессии T = 20 шагов, то есть агент, решающий задачу рекомендации, взаимодействует с пользователем в течении 20 шагов. Метрика качества данной задачи --- AverageReward\\

Для постановки и проведения эксперимента были проделаны следующие действия:
\begin{enumerate}
\item Произведена программная реализация рассмотренных графовых, а так же некоторых стандартных моделей.
\item Построена среда, имитирующая поведение пользователей и определена процедура обучения и тестирования моделей.
\item Модели были обучены согласно процедуре обучения.
\item Модели были протестированы, согласно процедуре тестирования, собраны метрики качества для проведения сравнительного анализа.
\end{enumerate}

По окончании эксперимента был проведен сравнительных анализ, сделаны выводы.

\section{Среда и процедура}
{\bf Среда.} Для проведения вычислительных экспериментов была простроена среда, имитирующая поведение реальных пользователей. В её основу были положены публичные датасеты: MovieLens, Goodreads, Steam. Каждый датасет представляет из себя последовательность взаимодействий пользователей с товарами, соответствующий результат взаимодействия и момент времени. Все результаты взаимодействий были приведены к значениям {0, 1}, где 0 --- отрицательное взаимодействие и 1 --- положительное. Так как длина сессии T = 20 шагов, из датасетов были удалены пользователи имеющие менее 20 товаров с положительным взаимодействием.\\

{\it MovieLens.} Датасет содержит оценки пользователей фильмам, собранные с апреля 2000 по февраль 2003. Оценки были приведены к {0, 1} следующим образом: 0 --- оценка $<$ 4, 1 --- оценка $\geq$ 4. Количество пользователей: 5041, количество товаров: 3458.\\

{\it Goodreads.} Оценки пользователей книгам, собранные с Февраля 2007 по
Ноябрь 2017. Оценки были приведены к {0, 1} слудующим образом: 0 --- оценка < 4, 1: оценка >= 4. Количество пользователей: 5717, количество товаров: 1500\\

{\it Steam.} Отзывы пользователей на игры, собранные с Октября 2010 по
Декабрь 2022. В качестве оценки использовался параметр is\_recommended: рекомендует ли пользователь данную игру, 0 --- не рекомендует, 1 --- рекомендует. Количество пользователей: 7008, Количество товаров: 2132\\

Поскольку в датасетах присутствуют не все возможные взаимодействия, доопределим результаты отсутствующих взаимодействий как отрицательные. Очевидно, что данное положение для некоторых товаров будет ошибочно, поэтому в предположении, что таких товаров относительно немного, решим эту проблему следующим образом: во время обучения на каждом шаге сессии действие, то есть следующий рекомендуемый пользователю товар, выбирается не из всего множества товаров, а из подмножества, состоящего из объединения множества товаров для которых в датасете имеется реальный результат взаимодействия с данным пользователем и 1000 случайно выбранных товаров, для которых результат взаимодействия был доопределен как отрицательный. Таким образом, если ложно-отрицательно-доопределенных товаров небольшое относительно истинно-отрицательно-доопределенных товаров количество, они редко будут попадать в подмножество, из которого рекоммендер выбирает положительный товар, соответственно, связанная с данным недостатком исскуственной среды ошибка рекоммендера не будет вносить большого смещения в результат обучения.\\

{\bf Процедура.} Для каждого датасета пять ряз выполняем следующую процедуру:
Разбиваем случайным образом с сидом, отличным от предыдущих, множество пользователей датасета на тренировочное, содержащее 80\% пользователей, и тестовое, содержащее 20\% пользователей.
Обучаем модели на тренировочном множестве пользователей в соответствии с алгоритмом
Тестируем модели на тестовом множестве пользователей в соответствии с алгоритмом
Собранные во время тестирования значения метрики качества усредняем по количеству выполнений процедуры, получаем итоговое значение метрики качества.

Для обучения были использованы 80\% пользоватей, для тестирования --- 20\%. Пользователи для обучения и тестирования выбирались случайно. Результаты были получены на 5ти различных случайных разбиениях датасета с различными сидами.


евсе такие товары не все ненаблюденные  действительно
отрицательны, мы случайным образом выбираем 1000 ненаблюдаемых (u,i) пар пользователей
u в качестве отрицательных обратных связей (yui = 0). В каждом эпизоде
взаимодействия пользователь-агент на T-шаге агент вынужден выбирать элементы из
доступного набора элементов, который состоит из 1000 отобранных отрицательных элементов
и наблюдаемых положительных элементов. Мы проводим эксперименты для сценария coldstart, который подразумевает, что агент не имеет данных обратной
связи от целевого пользователя на временном шаге t = 0, т.е. в начале каждого
Эпизод с Т-образным шагом. Мы разделяем каждый набор данных, случайным образом выбирая 80%
пользователи в качестве обучающего набора Ut r ain, а остальные 20% пользователей в качестве
тестового набора Utest . Мы проводим каждый эксперимент на 5 фрагментах данных
, полученных с использованием различных случайных начальных значений. Показатель оценки, который мы
использовали, - это среднее значение вознаграждений, полученных в эпизоде T-step, что
эквивалентно соотношению положительных элементов в рекомендуемых
элементах во время эпизода. Итоговый показатель получается путем вычисления его
среднего значения по всем тестируемым пользователям и разделения данных на 5


Для каждого датасета пять ряз выполняем следующую процедуру:
разбиваем случайным образом датасет на с сидом, отличным от предыдущих
Обучаем модели на трейне в соответствии с алгоритмом
Тестируем модели в соответствии с алгоритмом
Собранные во время тестирования значения метрики качества усредняем 

\section{Бейзлайн-модели}

asdfafa sdfasdf

\section{Результаты и анализ}

asdfafa sdfasdf


\chapter{Заключение}

Здесь идет текст. Вот так выглядит ссылка на библиографию \cite{langmuir26}. Аналогично добавляются еще главы, внутри них можно объявлять секции с помощью \verb|\section|.


\backmatter

%\printbib
% Следующие строки необходимо раскомментировать, а предыдущую закомментировать, если используется inline-библиография.
\begin{thebibliography}{99}
    \bibitem{langmuir26}
        H. Mott-Smith, I. Langmuir. ``The theory of collectors in gaseous discharges''. \emph{Phys. Rev.} \textbf{28} (1926)
\end{thebibliography}

\chapter{Благодарности}

Благодарности идут тут.

\end{document}