\documentclass[a4paper,14pt,oneside]{mipt-thesis-ms}
% Следующие две строки нужны только для biblatex. Для inline-библиографии их следует убрать.
%\usepackage{mipt-thesis-biblatex}
%\addbibresource{example.bib}

\usepackage{tabularx}
\usepackage[noend]{algorithmic}
\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
% переопределение стиля комментариев
\def\algorithmiccomment#1{\quad// {\sl #1}}

\title{Рекомендательные системы, основанные на графовых нейронных сетях}
\author{Минеев Н.\,Р.}
\supervisor{Киселёв Д.\,А.}
\referee{Петров Д.\,Е.}       % требуется только для mipt-thesis-ms
\groupnum{M05-114a}
\faculty{Физтех-школа Прикладной Математики и Информатики}
\department{Кафедра алгоритмов и технологий программирования}


% так можно определять команду для повторяющихся обозначений,
% чтобы не набирать каждый раз заново
\newcommand{\E}{\ensuremath{\mathsf{E}}}  % матожидание
\newcommand{\D}{\ensuremath{\mathsf{D}}}  % дисперсия
\newcommand{\Prb}{\ensuremath{\mathsf{P}}}  % вероятностная мера

\newcommand{\eps}{\varepsilon}  % нормальная буква эпсилон
\renewcommand{\phi}{\varphi}  % нормальная буква фи

\renewcommand{\le}{\leqslant}  % нормальный знак <=
\renewcommand{\leq}{\leqslant}  % нормальный знак <=
\renewcommand{\ge}{\geqslant}  % нормальный знак >=
\renewcommand{\geq}{\geqslant}  % нормальный знак >=

\newtheorem{lemma}{Лемма}  % создаёт команд для лемм, можно сделать так же для любого другого вида утверждений
\newtheorem{Def}{Определение}[section]

\begin{document}

\frontmatter
\titlecontents

\mainmatter

\newpage
{\large \bf Обозначения и сокращения}\\\\
MLP --- многослойный перцептрон \cite{haykin01}\\
RNN --- рекуррентная нейронная сеть \cite{sajid01}\\
LSTM --- долгая-краткосрочная память \cite{hochreiter01}\\
GRU --- управляемый рекуррентный блок. \cite{kyunghyun01}\\


\chapter{Введение}
{\bf Актуальность.} Задача рекомендации возникает во многих сферах человеческой жизни, от комерции и развлечений\cite{zangerle01}, до образования\cite{rivera01} и медицины\cite{tran01}, соответственно, высокого качества решение данной задачи позволяет улучшить моральное и физическое состояние людей, повысить их интелектуальное развитие и образование, углубить проффессиональные навыки, что в масштабах, например, государства приведет экономическому росту, национальной самообеспеченности и общественному развитию.

Рекомендательные системы - это особый тип систем фильтрации информации, целью которых является решение задачи рекомендации путем прогнозирования предпочтений пользователя в отношении того или иного товара/услуги\cite{peng01}. В основе прогнозирования лежат различные способы обработки истории взаимодействий пользователей с товарами, порождающие различные подходы к решению задачи, из которых наиболее результативными являются контентый и коллаборативная фильтрация.  
Контентный подход предполагает, что пользователю понравится товар, если он похож на понравившиеся данному пользователю товары ранее\cite{ricci01}. Коллаборативная фильтрация рассматривает процессы взаимодействий шире, включая, для прогнозирования предпочтений пользвателя, информацию о других похожих пользователях, предполагая, основываясь на схожести, что понравившиеся им товары понравятся и целевому пользователю\cite{chen02}. Так как система взаимодействий пользователей и товаров имеет графовую структуру, для извлечение информации возможно привлечение популярного сегодня и быстро развивающего в последние годы инструментария графовых нейронных сетей. Это позволяет рекомендательным системам эксплуатировать одновременно оба наиболее успешных подхода к решению задачи рекомендации, что выражается в повышении качества решения\cite{wu01}.

Для решения задачи рекомендации необходимо обучить модели успешно справляться с динамично развивающимися интересами и предпочтениями пользователей. Помимо этого рекомендательные системы способны непосредственно на данную динамику влиять, что приводит к изменениям самих таргетов для обучения и делает задачу в разы труднее в случае решения её в стандартной постановке обучения с учителем и приводит к решению в форме обучения с подкреплением. Обучение с подкреплением в случае задачи рекомендации имеет ряд преимуществ, например, возможность непрерывного обучения моделей в процессе их эксплуатации, оптимизация различных дополнительных целевых функций с помощью изменения функции награды и так далее\cite{chen03}.\\

{\bf Цели и задачи работы.}
В данной работе проведен анализ существующего графового метода для решения сессионной задачи рекомендации онлайн --- Graph Convolutional Q-Network (GCQN)\cite{lei01}, основанного на Graph Convolutional Recurrent Network (GCRN)\cite{seo01} фреймворке, найдены недостатки, в том числе, дискретный учет динамики графа взаимодействий пользователей и товаров, что может приводить к потере информации и снижению качества решения задачи рекомендации.\\

Целью работы является разработка нового метода, устраняюшего недостатки предыдущего, добавление непрерывного учета динамики графа взаимодействий пользователей и товаров, с помощью внедрения вместо, лежащего в основе предыдущего метода, Graph Convolutional Recurrent Network более совершенного Temporal Graph Network (TGN)\cite{rossi01} и проверка изменения качества решения сессионной задачи рекомендации онлайн. Для достжения поставленной цели необходимо решить следующие задачи:
\begin{enumerate}
\item Построить модель, основанную на TGN, для решения задачи рекомендации онлайн.
\item Провести численные эксперименты
\item Провести анализ результатов экспериментов, сравнить качество решения задачи рекомендации онлайн модели GCQN и предложенной модели.
\end{enumerate}


\chapter{Основные понятия и определения}

В данной главе определяются все необходимые понятия, описываются необходимые концепции, используемые повсеместно в работе и требующиеся для её полноценного понимания.


\section{Общая постановка задачи рекомендаций и методы решения}
{\bf Постановка задачи рекомендаций.} 
Задано множество пользователей $U = \{u_1, u_2, \dots, u_m\}$ и множество товаров $I = \{i_1, i_2, \dots, i_n\}$, и существует целевая функция $r: U \times I \times \mathcal{T} \rightarrow R$, выражающая результат взаимодействия пользователей с товарами, где $R$ --- линейно упорядоченное множество возможных результатов взаимодействия, $\mathcal{T}$ --- время.\cite{peng01}\\

Для краткости записи, введем следующие обозначения: \\
$r^t_{ui} = r(u, i, t) \in R$, \\
$r^t = r:\{t\} \times U \times I \rightarrow R$, где $t$ --- константа, \\
$r_{ui} = r: U \times I \rightarrow R$ --- отображение не зависит от $t \in \mathcal{T}$.\\

{\bf Задача рекомендации $K$ товаров:} По известной до момента времени $Т \in \mathcal{T}$ истории взаимодействий пользователей с товарами $\{u, i, t, r_{ui}^t\}^{t<T}_{u \in U,\:i \in I}$, необходимо 
\begin{enumerate}
    \item для момента времени $T$ построить аппроксимацию целевой функции $\hat r^T: U \times I \rightarrow R$;
    \item для каждого пользователя отранжировать товары по убыванию аппроксимированного результата взаимодействия в момент времени $Т$, взять первые $K$ товаров в качестве рекоммендованных.
\end{enumerate}

Для простоты, здесь и далее будем предполагать $К = 1$, а $R = \{0, 1\}$, где результат взаимодейтсвия $1$ будет интерпретироваться как положительный (понравилось, лайк, клик, покупка, итд), а $0$ как отрицательный (не понравилось, дизлайк, отказ от клика/покупки итд). Все приведенные в работе рассуждения несложно обобщаются на случай большего $К$ и большей мощности множества $R$, тем не менее оставим данное обобщение для будущих исследований.\\

{\it Для решения задачи рекомендации} применяют два основных подхода: контентный и коллаборативная фильтрация.
\\

{\bf Контентный подход решения задачи рекомендации.} В контентном подходе, пользователю рекоммендуются товары, похожие на понравившиеся ему ранее.\\

Пусть для каждого товара $i \in I$ имеется его признаковое описание $h_i \in \mathbb{R}^n$. Тогда для решения задачи рекомендации задаются функция аггрегации последовательности признаковых описаний товаров произвольной длины $l$, $agg: (\mathbb{R}^n)^l \rightarrow \mathbb{R}$ и, в пространстве признаковых описаний товаров, метрика $d: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$. \\
На первом шаге с помощью функции аггрегации строится профиль пользователя как аггрегация признаковых описаний товаров, с которыми данный пользователь провзаимодействовал $h_u = agg(\{h_i\})$. \\
На следующем шаге в качестве рекомендации пользователю выбирается товар, признаковое описание которого к профилю пользователя оказалось ближайшим: $i_{rec} = \arg \min_i d(h_u, h_i)$.\cite{lops01}\\

{\it Основное преимущество} контентного подхода в поддержке холодного старта для товаров и несложной интерпретируемости, объяснимости рекомендаций. {\it Недостаток} --- остутствие новизны: рекомендации основываются на похожих товарах.\\

{\bf Метод коллаборативной фильтрации для решения задачи рекомендации.} В случае коллаборативной фильтрации, пользователю рекоммендуются товары, понравившиеся похожим на данного пользователя пользователям.\\

Пусть для каждого товара $i \in I$ и пользователя $u \in U$ имеется признаковое описание, соответственно, $h_i \in \mathbb{R}^n$ и $h_u \in \mathbb{R}^n$. Тогда для решения задачи рекомендации определяется понятие окрестности пользователя $S(u)$ в пространстве $U$, а также задаются функция скоринга $score: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$, преобразующая пару признаковых описаний пользователя и товара в действительный число --- оценку, характеризующую привлекательность пользователю данного товара, и функция аггрегации последовательности оценок длины $l$, $agg: \mathbb{R}^l \rightarrow \mathbb{R}$. \\
На первом шаге с помощью функций скоринга и аггрегации оценок вычисляются приближенные рейтинги пользователя для каждого товара $\hat r_{ui} = {agg}_{v \in S(u)}(score(h_v, h_i))$. \\
На следующем шаге в качестве рекомендации пользователю выбирается товар, для которого приближенный рейтинг пользователя максимальный $i_{rec} = \arg \max_i \hat r_{ui}$. \cite{peng01}
\\

{\it Основное преимущество} коллаборативной фильтрации в более высоком качестве рекомендаций, но проблему холодного старта необходимо решать отдельно.
\\

{\bf Графовая структура задачи рекомендации.}\\
{\bf Граф} --- двойка, состоящая из множества вершин $V$ и множества ребер $E \subseteq V \times V$ --- пар элементов множества вершин: \:$G = <V, E>$.\\
{\bf Нейронные сети} --- метод машинного обучения, позволяющий извлекать информацию из сложноструктурированных данных, подобным человеческому мозгу образом.\\
{\bf Графовые нейронные сети (GNN)} --- тип нейронных сетей, позволяющих извлекать информацию из графов.\cite{gao01} \\

В основе графовых нейронных сетей лежит парадигма\\{\bf обмена сообщениями (message passing)\cite{gilmer01}}:
\\На нулевой итерации для каждой вершины графа $v \in V$ иницилизируется соответствующее векторное представление $x_v^0 \in \mathbb{R}^n$.
\\На каждой последующей итерации $t+1$:
\begin{enumerate}
    \item с помощью функции сообщений $msg$ для каждого ребра графа $e = (u, v) \in E$ вычисляется сообщение: $m_e^{t+1} = msg(x_u^t, x_v^t, w_e^t)$
    \item для каждой вершины графа $v \in V$ вычисляется обновленное векторное представление с помощью функции аггрегации сообщений от ребер $agg$, инцидентных данной вершине, и функции обновления $upd$: $x_v^{t+1} = upd(x_v^t, agg(\{m_e^{t+1}: (u, v) \in E\}))$
\end{enumerate}

Таким образом, по окончании $k$-итерационной процедуры векторные представления вершин содержат информацию о своих соседях до $k$го порядка. Часто, итерации называют слоями, а саму графовую нейронную сеть, как следствие, $k$-слойной.
\\

В силу того, что процесс взаимодействия пользователей и товаров имеет графовую структуру, а именно, представим в виде двудольного графа, который будем называть {\bf графом взаимодействий пользователей и товаров}, где вершины --- пользователи и товары, а ребра --- взаимодействия: $G = <U \cup I, r>$, для решения задачи рекомендации возможно применение графовых нейронных сетей следующим образом:
\begin{enumerate}
\item Необходимо с помощью графовой нейронной сети для графа взаимодействий пользователей и товаров вычислить векторные представления вершин, соответственно, векторные представления пользователей и товаров: $h_u$ и $h_i$.
\item С помощью скоринговой функции $score$ вычислить приближенные рейтинги пользователя для каждого товара $\hat r_{ui} = score(h_v, h_i)$.
\item в качестве рекомендации пользователю выбирается товар, для которого приближенный рейтинг пользователя максимальный $i_{rec} = \arg \max_i \hat r_{ui}$
\end{enumerate}

Таким образом, вычисленные с помощью графовых нейронных сетей на первом шаге, векторные представления пользователей, благодаря парадигме обмена сообщений, лежащей в основе GNN, будут содержать информацию как о товарах, с которыми провзаимодействовал данный пользователь, так и о других пользователях, которые также провзаимодействовали с данными товарами. То же и для товаров, их векторные представления будут содержать информацию как о пользователях, которые провзаимодействовали с этим товаром, так и о других товарах, с которыми данные пользователи также провзаимодейcтвовали.\\

Соответственно, {\it основное преимущество} решений задачи рекомендации, основанных на графовых нейронных сетях, является применение одновременно обоих подходов, контентного и коллаборативной фильтрации, благодаря лежащей в основе графовых нейронных сетей парадигме обмена сообщений.\\

{\it Важной особенностью} графа взаимодействий пользователей и товаров является зависимость его структуры от времени. Между пользователями и товарами возникают новые взаимодействия, количество пользователей и товарой может расти и убывать, что в структуре графа выражается в виде добавления/удаления ребер и вершин. Исходя из этого, для достижения выского качества решения задачи рекомендаций необходимо не только уметь извлекать информацию из графовой стркутуры, но и, для случая динамического графа, учитывать её динамику.

\section{Обучение представлений статических графов}
В случае графов, структура которых статична, для вычисления векторных представлений вершин используют статичные графовые нейронные сети \cite{kipf01, velicovic01, hamilton01, you01}. В их основе лежит описанная в предыдущей секции парадигма обмена сообщений\cite{gilmer01}, а различия, в основном, заключены в используемых функциях сообщений, агрегации и обновления. Рассмотрим статические графовые нейронные сети, используемые в данной работе.\\

{\bf Graph Convolutional Network \cite{kipf01}.} Для аггрегации информации от соседей вершины и итеративного обновления векторных представлений использует спектральное разложение Лапласиана графа:
$$m_{vu} = d_{vv}^{-\frac12}a_{vu}d_{uu}^{-\frac{1}{2}} h_u^l,$$
$$h_v^{l+1} = \sigma\left(W^l \sum_{u \in N(v)}m_{vu}\right),$$
где $\sigma(\cdot)$ --- нелинейная функция активации, например, relu, $W^l$ --- обучаемая матрица преобразования для слоя $l$, $a_{vu}$ --- вес смежности ($a_{vv} = 1$) и $d_{vv} = \sum_k a_{vk}$\\

{\bf GraphSAGE \cite{hamilton01}.} Авторами было предложено обобщение стандартных функций аггрегации в графовых сверточных сетях. В качестве аггрегатора можно использовать, например, mean/sum/max - pooling'и или рекуррентную нейронную сеть, вроде LSTM. Так же, в GraphSAGE аггрегация происходит не по всем соседям вершины, а по их случайному подмножеству фиксированного размера
$$m_{vu} = h_u^l,$$
$$n_v^l = {Aggregator}_l(\{h_u^l, \forall u \in N(v)\})$$
$$h_v^{l+1} = \sigma\left(W^l \cdot [h_v^l || n_v^l]\right),$$
где ${Aggregator}_l$ функция аггрегации на $l$-ом слое, $\sigma(\cdot)$ нелинейная функция активации, $W^l$ Обучаемая матрица преобразования и $||$ операция конкатенации.\\

{\bf Graph Attention Netowrk\cite{velicovic01}.} Делается предположение, что влияние каждого соседа может быть не одинаковым и информацию каждого соседа необходимо взвешивать. В таком случае, отличным решением является механизм внимания, где оценка внимания для каждого соседа будет характеризовать количество полезной информации, которое данный сосед может передать в векторное представление целевой вершины
$$m_{vu} = h_u^l,$$
$$n_v^l = \sum_{u \in N(v))}\alpha_{vu}h_u^l$$
$$\alpha_{vu} = \frac{\exp({Att}(h_v^l, h_u^l))}{\sum_{k \in N(v)} \exp({Att}(h_v^l, h_k^l))}$$
$$h_v^{l+1} = \sigma\left(W^l n_v^l\right),$$
где ${Att}(\cdot)$ функция внимания, обычно в качестве функции ${Att}(\cdot)$ берут ${LeakyReLU}(a^T[W^lh_v^l||W^lh_u^l])$, $\sigma(\cdot)$ нелинейная функция активации, $W^l$ Обучаемая матрица преобразования, $a^T$ Обучаемый вектор и $||$ операция конкатенации.

\section{Обучение представлений динамических графов}
Динамический граф --- граф, структура которого зависит от времени: $G(t) = <V(t), E(t)>,$($t~\in~\mathcal{T}$ --- время), то есть граф, в котором могут добавляться/удаляться вершины и ребра в различные моменты времени. Тогда для извлечения информации из графа такого рода, построения векторных представлений вершин, учитывающих динамику изменений, требуется определить способ представления данного графа и, в соответствии ему, фреймворк для извлечения информации.\cite{rossi01}\\

Существует два основных способа представления динамических графов: динамический граф дискретного времени (Discrete-time dynamic graphs, DTDG) и динамический граф непрерывного времени (Continious-time dynamic graphs, CTDG). Рассмотрим каждый из способов представления и соответствующие фреймворки для извлечения информации и построения векторных представлений вершин подробнее.\\

{\bf Динамический граф дискретного времени.} Пусть задана последовательность моментов времени $\{t_0, \dots, t_n\}$. Динамический граф дискретного времени представляет динамический граф в виде последовательности статичных графов, называемых снэпшотами, полученных с помощью фиксации динамического графа в соответствующий момент времени $G(t) = \{G^{t_0}, \dots, G^{t_n}\}, t_n \leq t$.\\

Для построения векторных представлений вершин динамического графа дискретного времени, учитывающих динамику, чаще всего используют различные вариации Graph Convolutional Recurrent Network (GCRN)-фреймворка\cite{seo01}.\\

Работу GCRN можно разбить на два основных этапа:
\begin{enumerate}
\item Для каждой вершины $i$ на каждом снэпшоте $G^{\tau}$ вычисляем статичное графовое векторное представление $h_i^{\tau}$ вершины $i$, путем применения некоторой статичной графовой нейронной сети.
\item Аггрегируя последовательность статичных векторных представлений $\{h_i^{t_0}, \dots, h_i^{t_n}\}$, с помощью некоторой функции аггрегации $agg: (\mathbb{R}^d)^{n+1} \rightarrow \mathbb{R}^d$, получают итоговое векторное представление $h_i$ вершины $i$, учитывающее динамику графа.
\end{enumerate}

В качестве аггрегатора используются mean/max-пулинги, свертки \cite{yu01, liu01, fathy01}, но чаще, как рассматривается и в данной работе, используют в качестве аггрегатора рекурентную нейронную сеть\cite{seo01, manessi01, li01, zhao01}, например, GRU, LSTM.\\

{\it Основным недостатком} представления динамического графа в виде динамического графа дискретного времени и использование соответствующих фреймворков для построения векторных представлений вершин данного графа, является возможная потеря информации при дискретизации времени и получении снэпшотов, что приводит к отсутствию прямого учета порядка и времени появления/удаления ребер и вершин.\\

{\bf Динамический граф непрерывного времени.} Пусть $x(t)$ событие на динамическом графе $G$ в момент времени $t$. Событие $x(t)$ может быть двух типов: 1) $x(t) = x_{vu}(t)$ --- добавление/удаление ребра между вершинами $v$ и $u$, 2) $x(t) = x_v(t)$ --- добавление/удаление вершины $v$. Тогда динамический граф непрерывного времени представляет динамический граф в виде последовательности событий $G(t) = \{x(t_1), x(t_2), \dots, x(t_n)\}, t_n \leq t$.\cite{rossi01}\\

Для построения векторных представлений вершин динамического графа непрерывного времени, Rossi et al. представили Temporal Graph Network (TGN) фреймворк\cite{rossi01}, обобщающий предыдущие модели для решения данной задачи: JODIE\cite{kumar01}, DyRep\cite{trivedi01}, TGAT\cite{xu01}.\\

TGN фреймворк состоит из пяти модулей: Memory, Message Function, Message Aggregator, Memory Updater, Embedding.
\begin{enumerate}
\item Memory модуль содержит векторное представление каждой вершины, называемое, вектором памяти, которое обновляется каждый раз, когда на графе происходит событие, в котором данная вершина участвует, что позволяет в векторе памяти хранить долговременную аггрегированную информацию об истории событий с данной вершиной.

Тем-не-менее, такое представление может приводить к проблеме <<застаревания памяти>>\cite{kazemi01}, для решения которой TGN имеет embedding-модуль.

\item Message Function --- функция, которая для каждой вершины участвующей в событии вычисляет вектор сообщения, используемый для обновления вектора памяти.

\item Message Aggregator --- функция, аггрегирующая несколько сообщений в одно. Так как для более эффективного обучения модели события могут быть объединены в батчи, возможна ситуация, когда в один батч попали несколько событий, в которых участвует одна и та же вершина, соответственно, для этой вершины получится несколько векторов сообщений, которые с помощью данного модуля аггрегируются в одно сообщений.

\item Memory Updater --- функция, обновляет вектор памяти вершины на основании полученного от нового события, в котором участвует данная вершина, вектора сообщения.

\item Embedding модуль вычисляет итоговые векторные представления для каждой вершины, учитывающие динамику графа.
\end{enumerate}

{\it Основное преимущество} представления динамического графа в виде динамического графа непрерывного времени и использование соответствующих фреймворков для построения векторных представлений вершин данного графа, является устранение недостатка динамического графа дискретного времени в возможной потери информации при дискретизации времени, что приводит к более высокому качеству решения различных задач, где необходима обработка динамических графов.\\


%\section{Обучение с подкреплением}


\chapter{Графовые модели для решения задачи рекомендаций онлайн}
Для решения задачи рекомендации необходимо обучить модели успешно справляться с динамично развивающимися интересами и предпочтениями пользователей. Помимо этого рекомендательные системы способны непосредственно на данную динамику влиять, что приводит к изменениям самих таргетов для обучения и делает задачу в разы труднее в случае решения её в стандартной постановке обучения с учителем и приводит к решению в форме обучения с подкреплением.\\

Преимуществом решения задачи рекомендации в форме обучения с подкреплением является, например, возможность обучения моделей в процессе их эксплуатации, в то же время к недостаткам можно отнести сложность построения хорошей искусственной среды для обучения моделей.

\section{Постановка задачи рекомендаций онлайн}
{\bf Постановка сессионной задачи рекомендации онлайн.} Пусть имеется множество пользователей $U = \{u_1, u_2, \dots, u_m\}$, множество товаров $I = \{i_1, i_2, \dots, i_n\}$ и существует функция $r:~U~\times~I~\times~\mathcal{T}~\rightarrow~\{0,  1\}$, выражающая результат взаимодействия пользователей с товарами так, что $r(u, i, t) = 0$ интерпретируется как <<пользователь $u$ в момент времени $t$ не удовлетворен товаром $i$>> (в то же время $r(u, i, t) = 1$ интерпретируется как <<пользователь $u$ в момент времени $t$ удовлетворен товаром $i$>>), где $\mathcal{T}$ --- время\cite{chen03}.\\
В случае сессионной постановки задачи рекомендации\cite{lei01}, когда $\mathcal{T} = \{0, 1, \dots, T - 1\}$, агент, решающий задачу рекомендации, взаимодействует с пользователем в течении $T$ временных шагов. Для каждого момента времени $t$: \\
\begin{enumerate}
\item агент наблюдает состояние $s_t$ пользователя $u$, которое представляется в виде последовательности товаров, с которыми провзаимодейтсвовал пользователь $u$ до момента времени $t$: $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ \\
\item на основании своей политики $\pi: I^t \rightarrow I$ агент выбирает действие $a_t \in I$, то есть следующий рекомендуемый пользователю товар. 
\item Далее, в момент времени $t+1$, в соответствии с выбранным действием $a_t$, агент получает награду ${reward}_t = r_t = r^{t}_{ui} = r(u, i, t)$ от пользователя $u$ и наблюдает новое состояние $s_{t+1} = \{a_0, a_1, \dots, a_{t}\}$ пользователя $u$. 
\end{enumerate}

Для оценки качества решения задачи онлайн рекомендации в сессионной постановке будем использовать {\bf среднюю награду за сессию (Average Reward)}: $AverageReward = \frac1T \sum_t r_t$.\\

Таким образом, {\bf сессионная задача рекомендации онлайн} формулируется следующим образом:\\
Пусть дано множество пользователей для обучения $U_{train} \subseteq U$. Необходимо по взаимодейтсвиям с пользователями из $U_{train}$ построить политику $\pi$, которая для любого нового пользователя $u \in U_{test} \subseteq U$ максимизирует среднюю награду за сессию $AverageReward \rightarrow \max$.

Для решения задачи будем использовать {\bf метод Q-обучения\cite{watkins01}}: \\Выразим политику через Q-функцию $\pi(s) = \arg \max Q^*(s, a)$. Тогда для решения задачи, необходимо построить аппроксимацию оптимальной Q-функции с помощью модели машинного обучения $Q^*(s, a) \approx Q_{\theta}(s, a)$, где $\theta$ -- параметры модели.


\section{Graph Convolutional Q-Network}
В 2020 году была предложена графовая модель для решения сессионной задачи рекомендации онлайн. Векторное представление состояния пользователя вычисляется с помощью модели, основанной на GCRN-фреймворке, а именно состояние пользователя $u$, представляемое в виде последовательности товаров, с которыми данный пользователь провзаимодействовал, преобразуется в последовательность графовых векторных представлений, полученных с помощью Graph Attention Netowork (GAT), примененной к графу взаимодействий пользователей и товаров, после чего данная последовательность аггрегируется с помощью рекуррентной нейронной сети Gated Recurrent Network (GRU), вычисляя таким образом векторное представление текущего состояния пользователя, учитывающее динамику графа взаимодействий пользователей и товаров. Далее рассматривается архитектура модели подробнее.
\\

{\bf Слой векторных представлений.} Отобразим каждого пользователя $u$ и товар $i$ в векторное пространство размерности $d$, таким образом получим, соответственно, векторные представления для пользователя $e_u \in \mathbb{R}^d$ и товара $e_i \in \mathbb{R}^d$. Данные векторные представления будем рассматривать как признаки вершин в графе, они будут случайно проициализированы в начале процесса обучения и обновляться вместе с другими параметрами модели.\\

{\bf GCN-слой.} Состояние $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ пользователя $u$ представим в виде динамического графа дискретного времени $G = \{G^0, G^1, \dots, G^{t-1}\}$, где $G^{\tau}$ --- снэпшот графа взаимодействий пользователей и товаров, взятый в момент времени $\tau$. Тогда для вычисления векторного представления $h_{s_t}$ состояния пользователя $u$, учитывающего структуру графа взаимодействий пользователей и товаров и его динамику, используем GCRN фреймворк, а векторные представления действий $h_a$ вычислим, применив статичную графовую нейронную сеть к последнему снэпшоту $G^{t-1}$.\\

На первом этапе GCRN к каждому снэпшоту применяется статичная графовая нейронная сеть, в случае GCQN применяется GAT. Для каждого товара $i$ найдем его векторное графовое представление $x_i \in \mathbb{R}^d$:
$$x_i = {relu}(W_{fc}[e_i || e_{N(i)}] + b_{fc}),$$
где $W_{fc} \in \mathbb{R}^{d \times 2d}$ и $b_{fc} \in \mathbb{R}^d$ обучаемые веса и смещения полносвязного слоя, $e_i$ векторное представление товара $i$, $||$ операция конкатенации, и $e_{N(i)} \in \mathbb{R}^d$ векторное представление соседей товара $i$:
$$e_{N(i)} = \sum_{w \in N(i)} \alpha_{iw}e_w,$$
где $N(i)$ множество соседей первого порядка товара $i$ в соответствующем снэпшоте графа взаимодействий пользователей и товаров $G^{\tau}$, $e_w$ векторное представление пользователя $w$, $\alpha_{iw}$ оценка внимания (attention score), определяющая количество информации, которое будет передано векторному представлению соседей товара $i$ от соседа $w$ и вычисляющаяся следующим образом: 
$$\alpha_{iw} = \frac{\exp(w_a^T\tanh(W_a[e_i || e_w]))}{\sum_{v \in N(i)} \exp(w_a^T \tanh(W_a[e_i || e_v]))},$$
где $W_a \in \mathbb{R}^{d \times 2d}$ и $w_a \in \mathbb{R}^d$ обучаемые веса механизма внимания, $\cdot^T$ операция транспонирования.

Таким образом, для каждого товара $a_{\tau} \in s_t$, берется соответствующий снэпшот графа $G_{\tau}$, для которого применяется процедура описанная выше и вычисляется векторное графовое представление $x_{a_{\tau}}$ товара $a_{\tau}$, а вся последовательность целиком $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ преобразуется в последовательность $x_{s_t} = \{x_{a_0}, x_{a_1}, \dots, x_{a_{t-1}}\}$.\\

В качестве векторного представления $h_{a_i}$ действия $a_i \in I$ используется векторное графовое представление $x_i \in \mathbb{R}^d$ товара $i$, полученное применением описанноей выше процедуры к последнему снэпшоту $G^t$.\\

{\bf GRU-слой.} На втором этапе GCRN, к последовательности графовых векторных представлений применяют операцию аггрегации, в случае GCQN применяют GRU, для учета динамики графа.

Последовательность $x_{s_t} = \{x_{a_0}, x_{a_1}, \dots, x_{a_{t-1}}\}$ преобразуют в последовательность $\{h_0, h_1, \dots h_{t-1}\}$, где $h_j \in \mathbb{R}^d$ скрытое состояние GRU на шаге $j$: $h_j = GRU(h_{j-1}, x_{a_j})$.
Последовательность скрытых состояний, в свою очередь, аггрегируется с помощью механизма внимания, таким образом получаем векторное представление $h_{s_t}$ состояния $s_t$:
$$h_{s_t} = \sum_{j=0}^{t-1} \beta_j h_j,$$
где $\beta_j$ оценки внимания, определяющия количество информации, которое будет передано векторному представлению состояния  $h_{s_t}$ от скрытого состояния $h_j$ и вычисляющиеся следующим образом:
$$\beta_j = \frac{\exp(w_{sa}^T\tanh(W_{sa}h_j))}{\sum_{l=0}^{t-1}\exp(w_{sa}^T\tanh(W_{sa}h_l))},$$
где $W_{sa} \in \mathbb{R}^{d \times d}$ и $w_a \in \mathbb{R}^d$ обучаемые веса механизма внимания.\\

{\bf MLP-слой.} Для вычисления итогового приближенного значения Q-функции используем многослойный перцептрон (MLP), который принимает на вход конкатенацию векторных представлений состояния $h_{s_t}$ и действия $h_{a_i}$ и возвращает приближенное значение $Q_{\theta}(s_t, a_i) \approx Q^*(s_t, a_i)$.\\

Благодаря тому, что в основе GCQN лежит GCRN фреймворк, модель обладает всеми {\it преимуществами} решений задачи рекомендации, основанных на графовых нейронных сетях. Но в модели существует {\it недостаток}, внесенный GCRN, а именно, дискретный учет динамики графа взаимодействий пользователей и товаров, из-за чего, во время разбиения графа на снэпшоты, возможна потеря важной информации, что приводит, в конечном итоге, к потере качества решения задачи рекомендации. В предлaгаемой далее модели предпринята попытка данный недостаток устранить.

\section{Предлагаемая модель}

Как было заключено в предыдущей секции, в основе модели GCQN для решения сессионной задачи рекомендации онлайн, лежит графовая модель GCRN, учитывающая динамику графа взаимодействий пользователей и товаров дискретно, что потенциально может приводить к потере информации во время дискретизации и снижению качества решения задачи рекомендации. Одним из возможных способов устранить данный недостаток является внедрение в модель TGN фреймворка, учитывающего динамику графа непрерывно, вместо GCRN. Таким образом была получена новая усовершенствованная модель --- TGQN. Далее рассмотрим архитектуру модели подробнее.
\\

{\bf Слой векторных представлений.} Отобразим каждого пользователя $u$ и товар $i$ в векторное пространство размерности $d$, таким образом получим, соответственно, векторные представления для пользователя $e_u \in \mathbb{R}^d$ и товара $e_i \in \mathbb{R}^d$. Данные векторные представления будем рассматривать как признаки вершин в графе, они будут случайно проициализированы в начале процесса обучения и обновляться вместе с другими параметрами модели.\\

{\bf TGN-слой.} Состояние $s_t = \{a_0, a_1, \dots, a_{t-1}\}$ пользователя $u$ представим в виде динамического графа непрерывного времени, то есть в виде последовательности событий $G = \{x_{u_0, i_0}(t_0), x_{u_1, i_1}(t_1), \dots, x_{u_n, i_n}(t_n)\}$, где $x_{u_k, i_k}(t_{k})$ --- событие взаимодействия в момент времени $t_k$ между пользователем $u_k$, возможно отличным от пользователя $u$, для состояния $s_t$ которого мы ищем векторное представление, и товаром $i_k$. Тогда для вычисления векторных представлений $h_{s_t}$ состояния пользователя $u$ и действий $h_a$, учитывающих структуру графа взаимодействий пользователей и товаров и его динамику, используем TGN фреймворк.\\

TGN фреймворк состоит из пяти модулей: Memory, Message Function, Message Aggregator, Memory Updater, Embedding. Рассмотрим работу каждого модуля по отдельности.\\

{\it Memory}-модуль. Для каждой вершины $v$ графа взаимодействий пользователей и товаров в момент времени $t$ memory-модуль содержит вектор памяти $s_{v}(t)$. Вектор памяти обновляется после каждого события на динамическом графе, таким образом, сохраняя в сжатом виде историю о событиях, в которых участвовала данная вершина $v$. В начале обучения все векторы памяти инициализируются нулями.\\

{\it Message Function}. Функция сообщений ${msg}$ вычисляет сообщение для каждой вершины $u$ и $i$, участвующей в событии $x_{u, i}(t)$, как конкатенацию их признаков $e_u \in \mathbb{R}^d$ и $e_i \in \mathbb{R}^d$ и векторного представления разности текущего момента времени $t$ и момента времени $t^-$ последнего события, в котором участвовала соответствующая вершина 
$$m_u(t) = [e_u || e_i || \phi(t - t^-_u)],$$
$$m_i(t) = [e_u || e_i || \phi(t - t^-_i)],$$
где $||$ --- операция конкатенации, $\phi(\cdot)$ --- функция, преобразующая разность моментов времени в вектор.\\

{\it Message Aggregator}. Так как в одном батче событий одна и та же вершина $v$ может встречаться несколько раз, соответственно, для неё будет несколько сообщений, которые необходимо аггрегировать с помощью $agg$ функции. В качестве $agg$ функции будем использовать max-pooling по времени, то есть просто вытаскивать самое последнее сообщение
$$\hat m_v(t) = \max_t(m_v(t_1), \dots, m_v(t_k))$$

{\it Memory Updater}. Данный модуль обновляет вектор памяти для вершины $v$, учавствующей в событии, на основе её аггрегированного сообщения $\hat m_v(t)$ и старого вектора памяти $s_v(t^-)$. В качестве memory updater'а будем использовать Gated Recurrent Network (GRU)
$$s_v(t) = GRU(\hat m_v(t), s_v(t^-))$$

{\it Embedding}-модуль. Для решения проблемы <<застаревания памяти>> (memory staleness problem) $s_v(t)$ используется дополнительный модуль, вычисляющий итоговое векторное представление $h_v(t) \in \mathbb{R}^d$ вершины графа $v$. В качестве embedding-модуля используем двуслойный Temporal Graph Attention: на вход каждый слой принимает представление $h_v^{l-1}(t)$ вершины $v$ с предыдущего слоя, текущий момент времени $t$, представления соседей вершины $v$ $\{h_1^{l-1}(t), \dots, h_N^{l-1}(t)\}$ вместе с моментами времени $t_1, \dots, t_N$
$$h_v^0(t) = [s_v(t) || e_v]$$
$$C^l(t) = [h_1^{l-1}(t)||\phi(t - t_1), \dots, h_N^{l-1}(t)||\phi(t - t_N)]$$
$$K^l(t) = V^l(t) = C^l(t)$$
$$q^l(t) = h_v^{l-1} || \phi(0)$$
$$\hat h_i^l(t) = MultiHeadAttention^l(q^l(t), K^l(t), V^l(t))$$
$$h_v^l(t) = MLP^l(h_v^{l-1}(t) || \hat h_v^l(t))$$
Здесь $\phi(\cdot)$ --- функция, преобразующая разность моментов времени в вектор, $||$ --- операция конкатенации, $MLP$ --- многослойный перцептрон.

Таким образом, выходные векторы $h_v^2(t) = h_v(t) \in \mathbb{R}^d$ с последнего слоя Temporal Graph Attention, примененного к графу взаимодействий пользователей и товаров, непрерывно учитывающие структуру графа и его динамику, будем использовать в качестве векторных представлений состояния $s_t = h_u(t)$ пользователя $u$ и, соответственно, действий $h_{a_i} = h_i$ товара $i$.\\

{\bf MLP-слой.} Для вычисления итогового приближенного значения Q-функции используем многослойный перцептрон (MLP), который принимает на вход конкатенацию векторных представлений состояния $h_{s_t}$ и действия $h_{a_i}$ и возвращает приближенное значение $Q_{\theta}(s_t, a_i) \approx Q^*(s_t, a_i)$.\\


Благодаря тому, что в основе TGQN лежит TGN фреймворк, модель обладает всеми {\it преимуществами} решений задачи рекомендации, основанных на графовых нейронных сетях. К тому же, модель {\it устраняет недостаток} предыдущего аналога GCQN, учитывая динамику графа взаимодействий пользователей и товаров непрерывно, не разбивая граф на снэпшоты, а обрабатывая непрерывный поток событий, что решает проблему потенциальной потери информации при дискретизации и на практике должно привести к приросту качества. Далее будет рассмотрен вычислительный эксперимент, и на основании проведенного сравнения и анализа сделанное выше предположение будет эмпирически проверено.

\chapter{Вычислительный эксперимент}
Для проверки предположения о том, что замена дискретного учета динамики графа взаимодействий пользователей и товаров на непрерывный приводит к росту качества решения задачи рекомендации, а также общей состоятельности предложенного метода относительно некоторых стандартных алгоритмов, был поставлен вычислительный эксперимент. В рамках эксперимента решалась сессионная задача рекомендации онлайн, с длиной сессии T = 20 шагов, то есть агент, решающий задачу рекомендации, взаимодействует с пользователем в течении 20 шагов. Метрика качества данной задачи --- AverageReward\\

Для постановки и проведения эксперимента были проделаны следующие действия:
\begin{enumerate}
\item Произведена программная реализация рассмотренных графовых, а так же некоторых стандартных моделей.
\item Построена среда, имитирующая поведение пользователей и определена процедура обучения и тестирования моделей.
\item Модели были обучены согласно процедуре обучения.
\item Модели были протестированы, согласно процедуре тестирования, собраны метрики качества для проведения сравнительного анализа.
\end{enumerate}

По окончании эксперимента был проведен сравнительных анализ, сделаны выводы.

\section{Среда и процедура}
{\bf Среда.} Для проведения вычислительных экспериментов была простроена среда, имитирующая поведение реальных пользователей. В её основу были положены публичные датасеты: MovieLens, Goodreads, Steam. Каждый датасет представляет из себя последовательность взаимодействий пользователей с товарами, соответствующий результат взаимодействия и момент времени. Все результаты взаимодействий были приведены к значениям {0, 1}, где 0 --- отрицательное взаимодействие и 1 --- положительное. Так как длина сессии T = 20 шагов, из датасетов были удалены пользователи имеющие менее 20 товаров с положительным взаимодействием.\\

{\it MovieLens\cite{movielens01}.} Датасет содержит оценки пользователей фильмам, собранные с апреля 2000 по февраль 2003. Оценки были приведены к {0, 1} следующим образом: 0 --- оценка $<$ 4, 1 --- оценка $\geq$ 4. Количество пользователей: 5041, количество товаров: 3458.\\

{\it Goodreads\cite{goodreads01}.} Оценки пользователей книгам, собранные с Февраля 2007 по
Ноябрь 2017. Оценки были приведены к {0, 1} слудующим образом: 0 --- оценка < 4, 1: оценка >= 4. Количество пользователей: 5717, количество товаров: 1500\\

{\it Steam\cite{steam01}.} Отзывы пользователей на игры, собранные с Октября 2010 по
Декабрь 2022. В качестве оценки использовался параметр is\_recommended: рекомендует ли пользователь данную игру, 0 --- не рекомендует, 1 --- рекомендует. Количество пользователей: 7008, Количество товаров: 2132\\

Поскольку в датасетах присутствуют не все возможные взаимодействия, доопределим результаты отсутствующих взаимодействий как отрицательные. Очевидно, что данное положение для некоторых товаров будет ошибочно, поэтому в предположении, что таких товаров относительно немного, решим эту проблему следующим образом: во время обучения на каждом шаге сессии действие, то есть следующий рекомендуемый пользователю товар, выбирается не из всего множества товаров, а из подмножества, состоящего из объединения множества товаров для которых в датасете имеется реальный результат взаимодействия с данным пользователем и 1000 случайно выбранных товаров, для которых результат взаимодействия был доопределен как отрицательный. Таким образом, если ложно-отрицательно-доопределенных товаров небольшое относительно истинно-отрицательно-доопределенных товаров количество, они редко будут попадать в подмножество, из которого рекоммендер выбирает положительный товар, соответственно, связанная с данным недостатком исскуственной среды ошибка рекоммендера не будет вносить большого смещения в результат обучения.\\

{\bf Процедура.} Для каждого датасета пять ряз выполняем следующую процедуру:
\begin{enumerate}
\item Разбиваем случайным образом с сидом, отличным от предыдущих, множество пользователей датасета $U$ на тренировочное $U_{train}$, содержащее 80\% пользователей, и тестовое $U_{test}$, содержащее 20\% пользователей.
\item Обучаем модели на тренировочном множестве пользователей в соответствии с алгоритмом
\begin{algorithmic}[1]
\REQUIRE $U_{train}$;
\ENSURE обученная модель;
\FOR{$session = 1, \dots, N$}
    \STATE Сэмплируем пользователя $u$ из $U_{train}$;
    \STATE Инициализируем состояние случайным товаром $s_0 = \{i_c\}$;
    \FOR{$t = 0, \dots, T-1$}
        \STATE выбираем действие $a_t$: с вероятностью $\epsilon$ случайный товар, иначе товар с наибольшим значением $Q_{\theta}(s_t, a)$;
        \STATE ${reward}_{t+1} = r(u, a_t, t)$;
        \STATE $s_{t+1} = s_t \cup {a_t}$;
        \STATE обновляем веса $\theta$ модели, используя DQNLoss;
    \ENDFOR
\ENDFOR
\end{algorithmic}

\item Тестируем модели на тестовом множестве пользователей в соответствии с алгоритмом
\begin{algorithmic}[1]
\REQUIRE $U_{test}$;
\ENSURE метрика качества AverageReward;
\STATE $AverageReward := 0$;
\FOR{$u \in U_{test}$}
    \STATE Инициализируем состояние случайным товаром $s_0 = \{i_c\}$
    \FOR{$t = 0, \dots, T-1$}
        \STATE выбираем действие $a_t$: товар с наибольшим значением $Q_{\theta}(s_t, a)$;
        \STATE ${reward}_{t+1} = r(u, a_t, t)$;
        \STATE $s_{t+1} = s_t \cup {a_t}$;
        \STATE $AverageReward += reward_{t+1} / T / |U_{test}|$;
    \ENDFOR
\ENDFOR
\end{algorithmic}

\end{enumerate}

Собранные во время тестирования значения метрики качества усредняем по количеству выполнений процедуры, получаем итоговое значение метрики качества.

\section{Бейзлайн-модели}

В дополнении к аналогичной предложенной модели GCQN, сравнения также были проведены с некоторыми стандартными алгоритмами решения задачи рекомендации.\\

{\bf Random.} Алгоритм на каждом шаге выбирает случайное действие независимо от состояния пользователя.\\

{\bf SVDQ\cite{zhang01}.} SVD обученный с помощью Q-обучения в соответствии приведенной выше процедуре, для решения сессионной задачи рекомендации онлайн. Векторные представления состояния и действий вычисляются через обучаемый слои векторных представлений.\\

{\bf GRUQ\cite{hidasi01}.} GRU4Rec обученный с помощью Q-обучения в соответствии приведенной выше процедуре, для решения сессионной задачи рекомендации онлайн. Вычисляет векторное представление состояния пользователя с помощью аггрегации послеодовательности товаров, с которыми пользователь провзаимодействовал, используя в качестве аггрегатора GRU модель. Векторыне представления действий вычисляются через обучаемый слой векторных представлений.\\

{\bf LSTMQ.} Модель аналогична GRUQ, но использует LSTM для аггрегации последовательности товаров, с которыми провзаимодействовал пользователь.\\


\section{Результаты и анализ}
Программную реализацию моделей и процедур обучения, тестирования можно найти в github-репозитории работы\cite{github01}.
\\

Значения метрики качества сессионной задачи рекомендации AverageReward приведены в Таблице 1. Как видно, предложенная
модель, TGQN, превосходит аналог, GCQN, на 10-30\%, что подтверждает предположение, о возможной потере информации во время дискретизации динамического графа. Предложенная модель также превосходит стандартные модели решения данной задачи, что говорит о состоятельности метода и возможности его применения на практике.\\


\begin{table}[]
\setlength\tabcolsep{0pt}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} cccccc }
\hline
              & MovieLens      & Goodreads      & Steam          \\ \hline
Random        & 0.077          & 0.024          & 0.037          \\ \hline
SVDQ          & 0.155          & 0.071           & 0.104          \\ \hline
GRUQ          & 0.283          & 0.108          & 0.133          \\ \hline
LSTMQ         & 0.273          & 0.117          & 0.155          \\ \hline
GCQN          & 0.300          & 0.122          & 0.179          \\ \hline
\textbf{TGQN} & \textbf{0.359} & \textbf{0.134} & \textbf{0.232} \\ \hline
\end{tabular*}
\caption{Значение метрики AverageReward для сессионной задачи рекомендации.}
\end{table}


\chapter{Заключение}
Работа посвящена исследованию графовых методов решения задачи рекомендации онлайн. Задача рекомендации сегодня возникает во многих сферах человеческой жизни и потому качественное её решение крайне востребовано. Решение задачи рекомендации в онлайн постановке с применением инструментария обучения с подкреплением обладает большим количеством преимуществ перед классической постановкой, в их числе обучение модели во время эксплуатации, гибкая функция награды, позволяющая во время обучения учитывать дополнительные целевые метрики и так далее. А использование графовых нейронных сетей для извлечения информации из истории взаимодействий пользователей с товарами, имеющей графовую структуру, позволяет основанным на них рекоммендерам одновременно эксплуатировать два мощных подхода к решению задачи рекомендации, что положительно отражается на качестве. 
В рамках работы был проведен анализ существующего графового метода решения задачи рекомендации онлайн --- Graph Convolutional Q-Network, найдены его недостатки, в том числе, дискретный учет динамики графа взаимодействий пользователей и товаров, который потенциально может приводить к потере важной информации, и, соответственно, снижать качество решения задачи. Была предпринята попытка устранения данного недостатка, путем внедрения, вместо используемого в GCQN для извлечения графовой информации фреймворка Graph Convolutional Recurrent Network, более совершенного, способного учитывать динамику графа непрерывно, фреймворка Temporal Graph Network. В результате была получена новая усовершенствованная модель Temporal Graph Q-Network, обладающая всеми преимуществами рекомендательных систем, основанных на графовых нейронных сетях, при этом учитывающая динамику графа взаимодействий пользователей и товаров непрерывно.

Temporal Graph Q-Network обладает всеми преимуществами рекомендательных систем, основанных на графовых нейронных сетях, при этом учитывающая динамику графа взаимодействий пользователей и товаров непрерывно. 

В результате численного эксперимента, было показано, что предложенная модель превосходит в метрике качества Average Reward в решении сессионной задачи рекомендации онлайн, как графовые аналоги, так и некоторые стандартные для данной задачи модели, что говорит о состоятельности метода и возможности его успешного применения на практике.

\backmatter

%\printbib
% Следующие строки необходимо раскомментировать, а предыдущую закомментировать, если используется inline-библиография.
\begin{thebibliography}{99}
    \bibitem{kipf01}
        N., Kipf T. Semi-Supervised Classification with Graph Convolutional Networks. — 2017.
    \bibitem{velicovic01}
        P., Veliˇckovi´c. Graph attention networks. — 2017.
    \bibitem{hamilton01}
        L., Hamilton W. Inductive representation learning on large graphs / Hamilton W. L., Ying R., Leskovec J. // Proceedings of IC on NIPS’17. — 2017.
— Pp. 1025–1035.
    \bibitem{gilmer01}
        Neural message passing for quantum chemistry / Justin Gilmer,
Samuel S Schoenholz, Patrick F Riley et al. // International conference
on machine learning / PMLR. — 2017. — Pp. 1263–1272.
    \bibitem{haykin01}
        Haykin, Simon. Neural networks: a comprehensive foundation / Simon Haykin. — Prentice Hall PTR, 1994.
    \bibitem{sajid01}
        Marhon, Sajid A. Recurrent Neural Networks / Sajid A. Marhon, Christopher J. F. Cameron, Stefan C. Kremer // Handbook on Neural Information
Processing / Ed. by Monica Bianchini, Marco Maggini, Lakhmi C. Jain.
— Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. — Pp. 29–65.
    \bibitem{hochreiter01}
        Hochreiter, Sepp. Long short-term memory / Sepp Hochreiter,
J¨urgen Schmidhuber // Neural computation. — 1997. — Vol. 9, no. 8.
— Pp. 1735–1780.
    \bibitem{kyunghyun01}
        Learning phrase representations using RNN encoder-decoder for statistical
machine translation / Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre et al. // arXiv preprint arXiv:1406.1078. — 2014.
    \bibitem{github01}
        Mineev, Nikita, Recommender systems based on graph neural netowrks, (2023), GitHub repository, https://github.com/nmineev/mipt-masters-project
    \bibitem{you01}
        J., You. Graphrnn: Generating realistic graphs with deep auto-regressive
models / You J., et al. // ICML’18 / PMLR. — 2018. — Pp. 5708–5717.
    \bibitem{seo01}
        Y., Seo. Structured Sequence Modeling with Graph Convolutional Recurrent Networks. — 2016.
    \bibitem{rossi01}
        E., Rossi. Temporal Graph Networks for Deep Learning on Dynamic
Graphs. — 2020.
    \bibitem{manessi01}
        F., Manessi. Dynamic graph convolutional networks / Manessi F., Rozza A., Manzo M. // Pattern Recognition. — 2020. — Vol. 97. — P. 107000.
    \bibitem{chen01}
        J., Chen. GC-LSTM: Graph Convolution Embedded LSTM for Dynamic
Link Prediction. — 2018.
    \bibitem{zhao01}
        L., Zhao. T-gcn: A temporal graph convolutional network for traffic prediction / Zhao L., et al. // IEEE ITSS. — 2019. — Vol. 21, no. 9. —
Pp. 3848–3858.
    \bibitem{li01}
        Y., Li. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. — 2018.
    \bibitem{yu01}
        B., Yu. Spatio-Temporal Graph Convolutional Networks: A Deep Learning
Framework for Traffic Forecasting / Yu B., Yin H., Zhu Z. // Proceedings
of IJCAI-18. — International Joint Conferences on Artificial Intelligence
Organization, 2018. — Pp. 3634–3640.
    \bibitem{fathy01}
        Fathy, A. TemporalGAT: Attention-Based Dynamic Graph Representation
Learning / A Fathy, et al. // Advances in Knowledge Discovery and Data
Mining. — Cham: Springer, 2020. — Pp. 413–423.
    \bibitem{liu01}
        Z., Liu. Motif-Preserving Dynamic Attributed Network Embedding /
Liu Z., et al. // Proceedings of the Web Conference 2021. — WWW
’21. — Association for Computing Machinery, 2021. — P. 1629–1638.
    \bibitem{trivedi01}
        R., Trivedi. DyRep: Learning Representations over Dynamic Graphs /
Trivedi R., et al. // ICLR (Poster). — LA: OpenReview.net, 2019.
    \bibitem{xu01}
        D., Xu. Inductive Representation Learning on Temporal Graphs. — 2020.
    \bibitem{kumar01}
        S., Kumar. Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks. — 2019.
    \bibitem{kazemi01}
        Representation Learning for Dynamic Graphs: A Survey. /
Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain et al. // J. Mach.
Learn. Res. — 2020. — Vol. 21, no. 70. — Pp. 1–73.
    \bibitem{peng01}
        Y., Peng. A Survey on Modern Recommendation System based on Big Data. — 2022.   
    \bibitem{rivera01}
        A. C., Rivera. Recommendation Systems in Education: A Systematic Mapping Study . — 2018.
    \bibitem{tran01}
        T., Tran. Recommender systems in the healthcare domain: state-of-the-art and research issues. — 2021.
    \bibitem{zangerle01}
        E., Zangerle. Evaluating Recommender Systems: Survey and Framework. — 2022
    \bibitem{ricci01}
        F.Ricci, L.Rokach, B.Shapira. Recommender Systems Handbook. Springer. 2015.
https://shuaizhang.tech/posts/2019/08/blog-post-2
    \bibitem{chen02}
        Rui Chen et al. A survey of collaborative ltering-based recommender systems: from traditional methods to hybrid methods based on social networks. 2018.
    \bibitem{wu01}
        S., Wu. Graph Neural Networks in Recommender Systems: A Survey. — 2022
    \bibitem{chen03}
        X., Chen. A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions. — 2021
    \bibitem{lei01}
        Y., Lei. Reinforcement Learning based Recommendation with Graph Convolutional Q-network. — 2020
    \bibitem{ko01}
        H., Ko. A Survey of Recommendation Systems: Recommendation Models, Techniques, and Application Fields. — 2022    
    \bibitem{lops01}
        P. Lops, M. d. Gemmis, and G. Semeraro, “Content-based recommender systems: State of the art and trends,”
Recommender systems handbook, pp. 73–105, 2011.
    \bibitem{gao01}
        C., Gao. A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions. — 2023
    \bibitem{gilmer01}
        Neural message passing for quantum chemistry / Justin Gilmer,
Samuel S Schoenholz, Patrick F Riley et al. // International conference
on machine learning / PMLR. — 2017. — Pp. 1263–1272.
    \bibitem{watkins01}
        C., Watkins. 1992. Q-learning. Machine learning 8, 3-4 (1992), 279–292.
    \bibitem{mnih01}
        Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,
Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement
learning. nature 518, 7540 (2015), 529–533
    \bibitem{movielens01}
        GroupLens Research. MovieLens 1M Dataset. — https://grouplens.org/datasets/movielens/1m/. — 2023. — June.
    \bibitem{steam01}
        Google LLC, Kaggle. [Weekly] 15M+ Game Recommendations on Steam. -- https://www.kaggle.com/datasets/antonkozyriev/game-recommendations-on-steam. -- 2023. -- June.
    \bibitem{goodreads01}
        M., Wan. Fine-grained spoiler detection from large-scale review corpora. -- https://cseweb.ucsd.edu/~jmcauley/datasets.html. -- 2019
    \bibitem{zhang01}
        Y., Zhang. An Introduction to Matrix factorization and Factorization Machines in Recommendation System, and Beyond. — 2022
    \bibitem{hidasi01}
        B., Hidasi. Session-based Recommendations with Recurrent Neural Networks. — 2015
    \bibitem{velicovic01}
        P., Veliˇckovi´c. Graph attention networks. — 2017
    
        
\end{thebibliography}


\end{document}